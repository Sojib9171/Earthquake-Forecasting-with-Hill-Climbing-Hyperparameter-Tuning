{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Libraries\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Input\n"
      ],
      "metadata": {
        "id": "JkIoommuVOug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1g9IYNBLkNHDNvnLPpKPXDs1hMUGsHkcJ'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8ChhBI50grHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess dataset\n",
        "df = df.dropna()  # remove missing values\n",
        "df = df[['Latitude', 'Longitude', 'Depth', 'Mag']]  # select useful columns"
      ],
      "metadata": {
        "id": "zhxuyUUqXzq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['Latitude', 'Longitude', 'Depth']].values\n",
        "y = df['Mag'].values"
      ],
      "metadata": {
        "id": "VdBVADvLX1M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Model"
      ],
      "metadata": {
        "id": "ni97da8C5iE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hXKWJKpSqXx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# User Settings\n",
        "user_hidden_layers = [16, 8]  # Default (modifiable)\n",
        "user_batch_size = 16\n",
        "user_learning_rate = 0.01\n",
        "\n",
        "feature_indices = {0: 'Latitude', 1: 'Longitude', 2: 'Depth'}\n",
        "all_features = [0, 1, 2]\n",
        "\n",
        "# Build Model Function\n",
        "def build_model(input_dim, hidden_layers, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))  # Add separate Input layer\n",
        "    model.add(Dense(hidden_layers[0], activation='relu'))  # First hidden layer\n",
        "    for layer_size in hidden_layers[1:]:\n",
        "        model.add(Dense(layer_size, activation='relu'))  # Additional hidden layers\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model = build_model(len(all_features), user_hidden_layers, user_learning_rate)\n",
        "\n",
        "# Train model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit(X_train, y_train, epochs=500, batch_size=user_batch_size,\n",
        "                    validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
        "\n",
        "# Evaluate model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Mean Absolute Error: {mae:.3f}\")\n",
        "\n",
        "#  Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#  Plot true vs predicted\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.xlabel(\"Actual Magnitude\")\n",
        "plt.ylabel(\"Predicted Magnitude\")\n",
        "plt.title(\"ANN Earthquake Magnitude Prediction\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection (Hill Climbing)\n",
        "print(\"Starting Feature Selection using Hill Climbing...\")\n",
        "\n",
        "def evaluate_features(features, X_train_full, X_test_full, y_train, y_test):\n",
        "    X_train_sub = X_train_full[:, features]\n",
        "    X_test_sub = X_test_full[:, features]\n",
        "\n",
        "    model = build_model(len(features), user_hidden_layers, user_learning_rate)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
        "    history = model.fit(X_train_sub, y_train, validation_split=0.2,\n",
        "                        epochs=200, batch_size=user_batch_size, verbose=0,\n",
        "                        callbacks=[early_stop])\n",
        "    loss, mae = model.evaluate(X_test_sub, y_test, verbose=0)\n",
        "    return loss, mae\n",
        "\n",
        "# Start random feature subset\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "current_features = random.sample(all_features, 2)\n",
        "print(f\"Initial random feature subset: {[feature_indices[i] for i in current_features]}\")\n",
        "\n",
        "best_loss, _ = evaluate_features(current_features, X_train, X_test, y_train, y_test)\n",
        "\n",
        "improved = True\n",
        "iteration = 1\n",
        "while improved:\n",
        "    improved = False\n",
        "    print(f\"\\nFeature Selection Iteration {iteration}\")\n",
        "    for f in all_features:\n",
        "        if f in current_features:\n",
        "            candidate = [feat for feat in current_features if feat != f]\n",
        "        else:\n",
        "            candidate = current_features + [f]\n",
        "\n",
        "        if len(candidate) == 0:\n",
        "            continue\n",
        "\n",
        "        candidate_loss, _ = evaluate_features(candidate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "        print(f\"Testing candidate features: {[feature_indices[i] for i in candidate]} - Loss: {candidate_loss:.4f}\")\n",
        "\n",
        "        if candidate_loss < best_loss:\n",
        "            print(\"Better feature subset found.\")\n",
        "            best_loss = candidate_loss\n",
        "            current_features = candidate\n",
        "            improved = True\n",
        "            break\n",
        "    iteration += 1\n",
        "\n",
        "best_features = current_features\n",
        "print(\"\\nBest feature subset selected:\", [feature_indices[i] for i in best_features])"
      ],
      "metadata": {
        "id": "iltbmVdvnR-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Hyperparameter Tuning (Hill Climbing)\n",
        "print(\"\\nStarting Hyperparameter Tuning using Hill Climbing...\")\n",
        "\n",
        "neurons_options = [8, 16, 32, 64]\n",
        "batch_size_options = [16, 32, 64]\n",
        "learning_rate_options = [0.001, 0.0005, 0.0001]\n",
        "\n",
        "def evaluate_hyperparameters(hidden_layers, batch_size, learning_rate, X_train_sub, X_test_sub, y_train, y_test):\n",
        "    model = build_model(X_train_sub.shape[1], hidden_layers, learning_rate)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
        "    history = model.fit(X_train_sub, y_train, validation_split=0.2,\n",
        "                        epochs=200, batch_size=batch_size, verbose=0,\n",
        "                        callbacks=[early_stop])\n",
        "    loss, mae = model.evaluate(X_test_sub, y_test, verbose=0)\n",
        "    return loss, mae\n",
        "\n",
        "X_train_sub = X_train[:, best_features]\n",
        "X_test_sub = X_test[:, best_features]\n",
        "\n",
        "# Start with current user settings\n",
        "current_hidden_layers = user_hidden_layers.copy()\n",
        "current_batch_size = user_batch_size\n",
        "current_learning_rate = user_learning_rate\n",
        "\n",
        "print(f\"Initial hyperparameters: Layers={current_hidden_layers}, Batch Size={current_batch_size}, Learning Rate={current_learning_rate}\")\n",
        "\n",
        "best_loss, _ = evaluate_hyperparameters(current_hidden_layers, current_batch_size, current_learning_rate, X_train_sub, X_test_sub, y_train, y_test)\n",
        "\n",
        "improved = True\n",
        "iteration = 1\n",
        "while improved:\n",
        "    improved = False\n",
        "    print(f\"\\nHyperparameter Tuning Iteration {iteration}\")\n",
        "    candidates = []\n",
        "\n",
        "    for i in range(len(current_hidden_layers)):\n",
        "        for neurons in neurons_options:\n",
        "            if neurons != current_hidden_layers[i]:\n",
        "                new_layers = current_hidden_layers.copy()\n",
        "                new_layers[i] = neurons\n",
        "                candidates.append((new_layers, current_batch_size, current_learning_rate))\n",
        "\n",
        "    for batch in batch_size_options:\n",
        "        if batch != current_batch_size:\n",
        "            candidates.append((current_hidden_layers, batch, current_learning_rate))\n",
        "\n",
        "    for lr in learning_rate_options:\n",
        "        if lr != current_learning_rate:\n",
        "            candidates.append((current_hidden_layers, current_batch_size, lr))\n",
        "\n",
        "    for hidden_layers_candidate, batch_candidate, lr_candidate in candidates:\n",
        "        candidate_loss, _ = evaluate_hyperparameters(hidden_layers_candidate, batch_candidate, lr_candidate, X_train_sub, X_test_sub, y_train, y_test)\n",
        "\n",
        "        print(f\"Testing: Layers={hidden_layers_candidate}, Batch={batch_candidate}, LR={lr_candidate} - Loss: {candidate_loss:.4f}\")\n",
        "\n",
        "        if candidate_loss < best_loss:\n",
        "            print(\"Better hyperparameter configuration found.\")\n",
        "            best_loss = candidate_loss\n",
        "            current_hidden_layers = hidden_layers_candidate\n",
        "            current_batch_size = batch_candidate\n",
        "            current_learning_rate = lr_candidate\n",
        "            improved = True\n",
        "            break\n",
        "    iteration += 1\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found:\")\n",
        "print(\"Hidden layers:\", current_hidden_layers)\n",
        "print(\"Batch size:\", current_batch_size)\n",
        "print(\"Learning rate:\", current_learning_rate)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jXMUA1ERGU-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Model Training\n",
        "print(\"\\nTraining Final Model with Best Settings...\")\n",
        "\n",
        "final_model = build_model(X_train_sub.shape[1], current_hidden_layers, current_learning_rate)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
        "\n",
        "final_history = final_model.fit(X_train_sub, y_train, validation_split=0.2,\n",
        "                                epochs=500, batch_size=current_batch_size,\n",
        "                                verbose=0, callbacks=[early_stop])\n",
        "\n",
        "final_loss, final_mae = final_model.evaluate(X_test_sub, y_test, verbose=0)\n",
        "print(f\"\\nFinal Test Mean Absolute Error (MAE): {final_mae:.3f}\")\n",
        "\n",
        "#  Plot True vs Predicted\n",
        "print(\"\\nPlotting True vs Predicted Earthquake Magnitudes...\")\n",
        "\n",
        "final_y_pred = final_model.predict(X_test_sub)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, final_y_pred, alpha=0.5)\n",
        "plt.xlabel(\"Actual Magnitude\")\n",
        "plt.ylabel(\"Predicted Magnitude\")\n",
        "plt.title(\"Final Optimized ANN: Earthquake Magnitude Prediction\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Curves\n",
        "print(\"\\nPlotting Training and Validation Loss Curves...\")\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(final_history.history['loss'], label='Training Loss')\n",
        "plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wAAdgvxCGT8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}